# How to do evaluate

- copy `eval_fn/` to your evaluation result folder, for example：`Multigen_Eval_history/grf-in_domain_eval` is one of result folder of Multigen, so it becomes：

        eval_fn/ ---> Multigen_Eval_history/grf-in_domain_eval/eval_fn/


- **Perplexity**：`cd eval_fn/`and evaluate with GPT-2：

    - `[--predict_out_file].txt` generated by two model are different
    - `source.csv` is copy from *OTTers dataset*
    - for Multigen：

            python3 ppl_test_preprocess.py --source_file ../source.csv --predict_out_file ../result_ep\:test.txt
            python3 perplexity.py ../ppl_input.txt
            

    - for T5-small：

            python3 ppl_test_preprocess.py --source_file ../source.csv --predict_out_file ../result_ep\:test.txt
            python3 perplexity.py ../ppl_input.txt


- **sacreBLEU**：

        pip install sacrebleu

        sacrebleu reference_target.txt -i prediction.txt -b -m bleu -w 3 --lowercase
    
    - **prediction.txt** can by either `result_ep:test.txt（Multigen）` or `generated_predictions.txt（T5-small）` in this repo.