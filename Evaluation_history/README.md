# How to do evaluate

- you can find `eval_fn/` under `Evaluation_history/`

- copy `eval_fn/` to your *result folder*, for example：`Multigen/models/in_domain/grf-in_domain_eval` is one of *result folder* of Multigen, so the file structure becomes：

        eval_fn/ ---> Multigen_Eval_history/grf-in_domain_eval/eval_fn/

- before evaluation make sure **Three** file in your *result folder*

		- source.csv
		- reference_target.txt
		- prediction.txt

- NOTE：
	- `source.csv` is from **OTTers dataset**
	- `reference_target.txt` is generated by `T5_small/train.py`
	- `prediction.txt` can be **Two** possible names in this repo.
		- `result_ep:test.txt` for Multigen
		- `generated_predictions.txt` for T5-small


- **Perplexity**：`cd [result folder]/eval_fn/` and evaluate with **GPT-2**：

    - for Multigen：

            python3 ppl_test_preprocess.py --source_file ../source.csv --predict_out_file ../result_ep\:test.txt
            python3 perplexity.py ../ppl_input.txt
            

    - for T5-small：

            python3 ppl_test_preprocess.py --source_file ../source.csv --predict_out_file ../generated_predictions.txt
            python3 perplexity.py ../ppl_input.txt


- **sacreBLEU**：`cd [result folder]/` and evaluate with **sacreBLEU**（`pip install sacrebleu`）

        sacrebleu reference_target.txt -i prediction.txt -b -m bleu -w 3 --lowercase